{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/balanced_test_set.csv'\n",
    "train_path = 'data/train_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;ID&gt;  &lt;ID&gt; Då missförstår du mig Hålla låg pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;ID&gt; om drivkraften bara är pengar så förstår jag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;ID&gt;  &lt;ID&gt; Här kan ni köpa in er i mitt andels...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Föredömligt av @FolksamMedia att inte invester...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rutger Arnhults privata bolag emitterar obliga...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  <ID>  <ID> Då missförstår du mig Hålla låg pro...          0\n",
       "1  <ID> om drivkraften bara är pengar så förstår jag          0\n",
       "2  <ID>  <ID> Här kan ni köpa in er i mitt andels...          0\n",
       "3  Föredömligt av @FolksamMedia att inte invester...          1\n",
       "4  Rutger Arnhults privata bolag emitterar obliga...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_path, delimiter=',', quotechar='|', encoding=\"utf8\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;ID&gt;  &lt;ID&gt; jaaa med Calle och Hobbe p</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÅH VAD JAG ÄLSKAR ATT VARA KVINNA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drygt en vecka kvar rösta i kyrkovalet gärna p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En lite bra tråd om våld och nazister och sånt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>På väg hem efter en fantastiskt trevlig AW med...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0              <ID>  <ID> jaaa med Calle och Hobbe p          1\n",
       "1                  ÅH VAD JAG ÄLSKAR ATT VARA KVINNA          1\n",
       "2  Drygt en vecka kvar rösta i kyrkovalet gärna p...          1\n",
       "3  En lite bra tråd om våld och nazister och sånt...          1\n",
       "4  På väg hem efter en fantastiskt trevlig AW med...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(test_path, delimiter=',', quotechar='|', encoding=\"utf8\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "### Do we want to under-sample the majority classes, creating a balanced training set where randomly selected tweets from the majority classes are extracted to match the minority class? Or do we want to under-sample the majority class(es) and over-sample the minority class(es)\n",
    "\n",
    "#### -1 = negative  |  0 = neutral  |  1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Tweet\n",
       "Sentiment       \n",
       "-1          2551\n",
       " 0          5820\n",
       " 1          2703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('Sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under- and over-sampling to create a balanced training set with 3100 tweets in each class\n"
     ]
    }
   ],
   "source": [
    "# ---- Enter True / False ----\n",
    "# ---- if False, new varible must be larger than smallest and smaller than largest in table above. -----\n",
    "only_under_sample = False\n",
    "no_sampling = False # use ALL training data without any balancing.\n",
    "\n",
    "new = 3100\n",
    "\n",
    "if only_under_sample:\n",
    "    print('Under-sampling, only.')\n",
    "else:\n",
    "    print('Under- and over-sampling to create a balanced training set with {} tweets in each class'.format(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under- and over-sampling a balanced training data set\n",
      "Shuffling new dataset...\n",
      "Size of training set, with 3100 in each class:\n",
      "9300\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "neu = []\n",
    "for tweet_tuple in train_data.iterrows():\n",
    "    tweet = tweet_tuple[1]['Tweet']\n",
    "    sentiment = int(tweet_tuple[1]['Sentiment'])\n",
    "\n",
    "    if sentiment == -1:\n",
    "        neg.append([tweet, sentiment])\n",
    "\n",
    "    if sentiment == 0:\n",
    "        neu.append([tweet, sentiment])\n",
    "\n",
    "    if sentiment == 1:\n",
    "        pos.append([tweet, sentiment])\n",
    "            \n",
    "len_neu = len(neu)\n",
    "len_neg = len(neg)\n",
    "len_pos = len(pos)\n",
    "            \n",
    "#print(len_neu, ' Neutral')\n",
    "#print(len_neg, ' Negative')\n",
    "#print(len_pos, ' Positive')\n",
    "\n",
    "minimi = min(len_neg, len_neu, len_pos)\n",
    "\n",
    "if only_under_sample and not no_sampling:\n",
    "    print('Under-sampling a balanced training data set')\n",
    "    \n",
    "    print('Smallest class size: ', minimi)\n",
    "\n",
    "    np.random.shuffle(pos)\n",
    "    np.random.shuffle(neg)\n",
    "    np.random.shuffle(neu)\n",
    "    \n",
    "    print('Shuffling new dataset...')\n",
    "    balanced_data = np.vstack((pos[:minimi], neu[:minimi], neg[:minimi]))\n",
    "\n",
    "    np.random.shuffle(balanced_data)\n",
    "    print('Size of training set, with {} in each class:'.format(minimi))\n",
    "    print(len(balanced_data))\n",
    "    \n",
    "elif not only_under_sample and not no_sampling:\n",
    "    print('Under- and over-sampling a balanced training data set')\n",
    "    \n",
    "    remaining_pos = new - len(pos)\n",
    "    remaining_neg = new - len(neg)\n",
    "\n",
    "    pos_copy = pos.copy()\n",
    "    neg_copy = neg.copy()\n",
    "\n",
    "    upsampled_pos = []\n",
    "    upsampled_neg = []\n",
    "\n",
    "\n",
    "    for i in range(remaining_pos):\n",
    "        rand_id = random.randint(0, len(pos_copy) - 1)\n",
    "        upsampled_pos.append(pos_copy.pop(rand_id))\n",
    "\n",
    "    for i in range(remaining_neg):\n",
    "        rand_id = random.randint(0, len(neg_copy) - 1)\n",
    "        upsampled_neg.append(neg_copy.pop(rand_id))\n",
    "        \n",
    "    pos = np.vstack((pos, upsampled_pos))\n",
    "    neg = np.vstack((neg, upsampled_neg))\n",
    "\n",
    "    np.random.shuffle(pos)\n",
    "    np.random.shuffle(neg)\n",
    "    np.random.shuffle(neu)\n",
    "    \n",
    "    # Note that this line must be changed if the distribution of the training data shifts.\n",
    "    # It now assumes that pos and neg is over-sampled and that neu is under-sampled!\n",
    "    balanced_data = np.vstack((pos, neu[:new], neg))\n",
    "    \n",
    "    print('Shuffling new dataset...')\n",
    "    np.random.shuffle(balanced_data)\n",
    "    print('Size of training set, with {} in each class:'.format(new))\n",
    "    print(len(balanced_data))\n",
    "    \n",
    "elif not only_under_sample and no_sampling:\n",
    "    balanced_data = np.vstack((pos, neu, neg))\n",
    "    print('Including ALL available training data <-- unbalanced')\n",
    "    np.random.shuffle(balanced_data)\n",
    "    print('size:')\n",
    "    print(len(balanced_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate tweets from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [pair[0] for pair in balanced_data]\n",
    "train_Y = [int(pair[1]) for pair in balanced_data]\n",
    "\n",
    "test_X = test_data['Tweet'].tolist()\n",
    "test_Y = test_data['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    '''\n",
    "    An ad-hoc spellcheck. No swedish words contains more than two identical characters\n",
    "    in a sequence. This method finds those and shortens them to two - irregardless of\n",
    "    whether it's a noun or whatever. ALL occurrences are reduced to only two in order to\n",
    "    ensure words that are commonly emphazised, i.e såååååå, are included in the vocabulary.\n",
    "    '''\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and pre-process each tweet. Stemming and stop-word removal is removed from pre-processing stage after empirical findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    \n",
    "    # Here we have each tweet. Process and add to file.\n",
    "\n",
    "    # Replaces various URLs with \"<URL>\"\n",
    "    filtered = re.sub(r\"http\\S+\", \" <URL>\", tweet)\n",
    "\n",
    "    # Replace mentions with anonymous <ID>\n",
    "    filtered = re.sub(r\"@\\S+\", \" <ID>\", filtered)\n",
    "\n",
    "    # Replace haschtags with anonymous <HASCHTAG>\n",
    "    filtered = re.sub(r\"#\\S+\", \" <HASCHTAG>\", filtered)\n",
    "\n",
    "    filtered = reduce_lengthening(filtered)\n",
    "\n",
    "    # Replace each '-' and '/' with ' - ' and ' / ' because they are common in text\n",
    "    filtered = filtered.replace(\"-\", \" - \")\n",
    "    filtered = filtered.replace(\"/\", \" / \")\n",
    "\n",
    "    filtered = re.sub('[\\'&]+', '', filtered)\n",
    "\n",
    "    #Remove non-alpha numerical and <> from tweet\n",
    "    filtered = re.sub('[^0-9a-zA-Z åäöÅÄÖ<>]+', ' ', filtered)\n",
    "\n",
    "    prev = ''\n",
    "    sentence = ''\n",
    "    for word in filtered.split():\n",
    "        word = word.strip().lower()\n",
    "\n",
    "        if not (word == prev and (word == '<id>' or word == '<haschtag>' or word == '<url>')) and not word == 'amp':\n",
    "            sentence = sentence + ' ' + word\n",
    "        prev = word\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "train_cleaned_tweets = []\n",
    "for tweet in train_X:\n",
    "    tweet = cleanTweet(tweet)\n",
    "    #print(tweet)\n",
    "    train_cleaned_tweets.append(tweet)\n",
    "    \n",
    "test_cleaned_tweets = []\n",
    "for tweet in test_X:\n",
    "    tweet = cleanTweet(tweet)\n",
    "    #print(tweet)\n",
    "    test_cleaned_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure alll dimensions add up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9300 9300\n",
      "1008 1008\n"
     ]
    }
   ],
   "source": [
    "print(len(train_cleaned_tweets), len(train_Y))\n",
    "print(len(test_cleaned_tweets), len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabularyBigram(text):\n",
    "    bi_dic = {}\n",
    "    for line in text:\n",
    "        sentence = line.split()\n",
    "        for ix, word in enumerate(sentence):\n",
    "            try:\n",
    "                if sentence[ix + 1]:\n",
    "                    word = word.strip() #Removes blanks and '\\n' from the end when needed.\n",
    "                    next_word = sentence[ix + 1].strip()\n",
    "                    bi_gram = word + ' ' + next_word\n",
    "                    bi_dic[bi_gram] = bi_dic.get(bi_gram, 0) + 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return bi_dic\n",
    "bigram_vocabulary = createVocabularyBigram(train_cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams:\n",
      "71652\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique bigrams:')\n",
    "print(len(bigram_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2index is a dictionary that maps a word to an index, i.e corresponding place in input vector\n",
    "### index2word is a dictionary that maps an index (from the input vector-space) to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def createWordMapper(vocabulary, vocabSize):\n",
    "    '''\n",
    "    Takes a vocabulary (dictionary) of variable size and a desired vocabulary size as input. \n",
    "    Returns the two mappers: word2index & index2word.\n",
    "    \n",
    "    NB, when the mappers are created we don't use the original vocabulary anymore (i think).\n",
    "    '''\n",
    "\n",
    "    # Order the dictinoary in order to extract the most frequent\n",
    "    sortedVocabulary = sorted(vocabulary.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    word2index = collections.OrderedDict()\n",
    "    index2word = collections.OrderedDict()\n",
    "\n",
    "    for i in range(vocabSize):\n",
    "        index = len(word2index)\n",
    "        word = sortedVocabulary[i][0]\n",
    "        \n",
    "        word2index[word] = index\n",
    "        index2word[index] = word\n",
    "    word2index['UNK'] = index + 1\n",
    "    index2word[index + 1] = 'UNK'\n",
    "            \n",
    "    return word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 30000\n",
      "Maximum size of vocabulary: 71652\n"
     ]
    }
   ],
   "source": [
    "# vocabularySize = len(bigram_vocabulary) # Can't use this because memory overflow, but we want to!!!\n",
    "\n",
    "vocabularySize = 30000\n",
    "\n",
    "bigram2index, index2bigram = createWordMapper(bigram_vocabulary, vocabularySize - 1)\n",
    "print('Size of vocabulary: ' +str(len(bigram2index)))\n",
    "print('Maximum size of vocabulary: ' + str(len(bigram_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  383\n",
      "word:  jag gillar\n"
     ]
    }
   ],
   "source": [
    "test = bigram2index.get('jag gillar')\n",
    "print('index: ', test)\n",
    "print('word: ', index2bigram.get(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input vectors with BoW approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBigramVector(tweet):\n",
    "    vec = np.zeros([vocabularySize, 1])\n",
    "    sentence = tweet.split()\n",
    "    for ix, word in enumerate(sentence):\n",
    "        \n",
    "        try:\n",
    "            if sentence[ix + 1]:\n",
    "                word = word.strip() #Removes blanks and '\\n' from the end when needed.\n",
    "                next_word = sentence[ix + 1].strip()\n",
    "                bi_gram = word + ' ' + next_word\n",
    "                \n",
    "                if bigram2index.get(bi_gram) is not None:\n",
    "                    vec[bigram2index.get(bi_gram)] += 1\n",
    "                    \n",
    "                else:\n",
    "                    vec[bigram2index.get('UNK')] += 1\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return vec\n",
    "\n",
    "            \n",
    "train_x_vectorized = []\n",
    "for tweet in train_cleaned_tweets:\n",
    "    train_x_vectorized.append(getBigramVector(tweet))\n",
    "    \n",
    "test_x_vectorized = []\n",
    "for tweet in test_cleaned_tweets:\n",
    "    test_x_vectorized.append(getBigramVector(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "del test_data\n",
    "del pos\n",
    "del neg\n",
    "del neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary 1-dimension which np creates above\n",
    "train_x_vectorized = np.squeeze(train_x_vectorized)\n",
    "test_x_vectorized = np.squeeze(test_x_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:38: UserWarning: [Errno 12] Cannot allocate memory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_linear = LinearSVC(C=1, max_iter = 10000, verbose=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_linear.fit(train_x_vectorized, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.37      0.45       336\n",
      "           0       0.43      0.65      0.52       336\n",
      "           1       0.58      0.48      0.52       336\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      1008\n",
      "   macro avg       0.52      0.50      0.50      1008\n",
      "weighted avg       0.52      0.50      0.50      1008\n",
      "\n",
      "\n",
      "Accuracy on test/evaluation set:  0.5\n",
      "\n",
      "Confusion matrix:\n",
      "[[124 154  58]\n",
      " [ 58 219  59]\n",
      " [ 38 137 161]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "y_true, y_pred = test_Y, clf_linear.predict(test_x_vectorized)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "print('Accuracy on test/evaluation set: ', accuracy_score(y_true, y_pred))\n",
    "print()\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a calibrated classifier using platt-scaling, to produce probabilities per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-724c13044029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf_calibrated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf_calibrated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_calibrated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/calibration.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m                         sample_weight=base_estimator_sample_weight[train])\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mthis_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 calibrated_classifier = _CalibratedClassifier(\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "clf_calibrated = CalibratedClassifierCV(clf_linear, cv=5) \n",
    "clf_calibrated.fit(train_x_vectorized, train_Y)\n",
    "y_proba = clf_calibrated.predict_proba(test_x_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print tweet, probabilities and prediction per tweet in test set\n",
    "#### Probabilities: [prob_neg, prob_neu, prob_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(test_X):\n",
    "    label = y_true[index]\n",
    "    pred = y_pred[index]\n",
    "\n",
    "    print()\n",
    "    print('Original tweet:')\n",
    "    print(i)\n",
    "    print()\n",
    "    print('Preprocessed tweet:')\n",
    "    print(test_cleaned_tweets[index])\n",
    "    print()\n",
    "    print('Probabilities:')\n",
    "    print('    neg \\t neu \\t   pos')\n",
    "    print(y_proba[index])\n",
    "    print()\n",
    "    print('Prediction: \\t', pred)\n",
    "    print('label \\t\\t', label)\n",
    "    print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like above, but only the missclassified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, i in enumerate(test_X):\n",
    "    label = y_true[index]\n",
    "    pred = y_pred[index]\n",
    "    if not (label == pred):\n",
    "        count += 1\n",
    "        print()\n",
    "        print('Original tweet:')\n",
    "        print(i)\n",
    "        print()\n",
    "        print('Preprocessed tweet:')\n",
    "        print(test_cleaned_tweets[index])\n",
    "        print()\n",
    "        print('Probabilities:')\n",
    "        print('    neg \\t neu \\t   pos')\n",
    "        print(y_proba[index])\n",
    "        print()\n",
    "        print('Prediction: \\t', pred)\n",
    "        print('label \\t\\t', label)\n",
    "        print()\n",
    "        print('-------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "### The two crucial parts are the model itself and the word2index mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_model(path, model, prob_model, w2i):\n",
    "    dir_name = path.split('.')[0]\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    else:\n",
    "        print('Error: \\nChoose different model name for saving, as a folder with that name already exists')\n",
    "        return\n",
    "    \n",
    "    model_path = os.path.join(dir_name, path)\n",
    "    prob_model_path = os.path.join(dir_name, 'prob_'+path)\n",
    "    w2i_path = os.path.join(dir_name, 'word2index.pkl')\n",
    "    init_path = os.path.join(dir_name, '__init__.py')\n",
    "    \n",
    "    save_pickle(model_path, model)\n",
    "    save_pickle(prob_model_path, prob_model)\n",
    "    save_pickle(w2i_path, w2i)\n",
    "    \n",
    "    f = open(init_path, 'w')\n",
    "    f.close()\n",
    "    \n",
    "def save_pickle(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to save the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'bigram_linear_svm.pkl' # Must be a .pkl file extension name!\n",
    "\n",
    "\n",
    "save_model(model_save_path, clf_linear, clf_calibrated, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF-kernel SVM (Non-linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(C=15, gamma=1e-2, verbose=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc.fit(train_x_vectorized, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "y_true, y_pred = test_Y, clf_svc.predict(test_x_vectorized)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "print('Accuracy on test/evaluation set: ', accuracy_score(y_true, y_pred))\n",
    "print()\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

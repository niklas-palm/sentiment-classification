{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to a .csv with the tweets to learn the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePath = 'data/ALLtweets_2017.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log_progress is a helper method for diplaying a progression bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    '''\n",
    "    An ad-hoc spellcheck. No swedish words contains more than two identical characters\n",
    "    in a sequence. This method finds those and shortens them to two - irregardless of\n",
    "    whether it's a noun or whatever. ALL occurrences are reduced to only two in order to\n",
    "    ensure words that are commonly emphazised, i.e såååååå, are included in the vocabulary.\n",
    "    '''\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e80d2ee517e4b778bc9f45e859e2103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=13904701)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cleanAllTweets(sourcePath):    \n",
    "    cleaned_tweets = []\n",
    "    tweets = pd.read_csv(sourcePath, delimiter=',', quotechar='|' )['Tweet']\n",
    "    size = len(tweets)\n",
    "    for tweet in log_progress(tweets, size = size):\n",
    "        tweet = cleanTweet(tweet)\n",
    "        cleaned_tweets.append(tweet)\n",
    "    \n",
    "    return cleaned_tweets\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "    \n",
    "    # Here we have each tweet. Process and add to file.\n",
    "\n",
    "    # Replaces various URLs with \"<URL>\"\n",
    "    filtered = re.sub(r\"http\\S+\", \" <URL>\", tweet)\n",
    "\n",
    "    # Replace mentions with anonymous <ID>\n",
    "    filtered = re.sub(r\"@\\S+\", \" <ID>\", filtered)\n",
    "\n",
    "    # Replace haschtags with anonymous <HASCHTAG>\n",
    "    filtered = re.sub(r\"#\\S+\", \" <HASCHTAG>\", filtered)\n",
    "\n",
    "    filtered = reduce_lengthening(filtered)\n",
    "\n",
    "    # Replace each '-' and '/' with ' - ' and ' / ' because they are common in text\n",
    "    filtered = filtered.replace(\"-\", \" - \")\n",
    "    filtered = filtered.replace(\"/\", \" / \")\n",
    "\n",
    "    filtered = re.sub('[\\'&]+', '', filtered)\n",
    "\n",
    "    #Remove non-alpha numerical and <> from tweet\n",
    "    filtered = re.sub('[^0-9a-zA-Z åäöÅÄÖ<>]+', ' ', filtered)\n",
    "\n",
    "    prev = ''\n",
    "    sentence = ''\n",
    "    for word in filtered.split():\n",
    "        word = word.strip().lower()\n",
    "\n",
    "        if not (word == prev and (word == '<id>' or word == '<haschtag>' or word == '<url>')) and not word == 'amp':\n",
    "            sentence = sentence + ' ' + word\n",
    "        prev = word\n",
    "\n",
    "    return sentence\n",
    "\n",
    "cleaned_tweets = cleanAllTweets(sourcePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabulary(tweets):\n",
    "    size = len(tweets)\n",
    "    dic = {}\n",
    "    for tweet in log_progress(tweets, size=size):\n",
    "        for word in tweet.split():\n",
    "            word = word.strip() #Removes blanks and '\\n' from the end when needed.\n",
    "            dic[word] = dic.get(word, 0) + 1\n",
    "                \n",
    "    return dic\n",
    "vocabulary = createVocabulary(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the word2index and index2word mappers. These ensures that we can map a word to an index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def createWordMapper(vocabulary, vocabSize):\n",
    "    '''\n",
    "    Takes a vocabulary (dictionary) of variable size and a desired vocabulary size as input. \n",
    "    Returns the two mappers: word2index & index2word.\n",
    "    \n",
    "    NB, when the mappers are created we don't use the original vocabulary anymore (i think).\n",
    "    '''\n",
    "\n",
    "    # Order the dictinoary in order to extract the most frequent\n",
    "    sortedVocabulary = sorted(vocabulary.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    word2index = collections.OrderedDict()\n",
    "    index2word = collections.OrderedDict()\n",
    "\n",
    "    for i in log_progress(range(vocabSize), size=vocabSize):\n",
    "        index = len(word2index)\n",
    "        word = sortedVocabulary[i][0]\n",
    "        \n",
    "        word2index[word] = index\n",
    "        index2word[index] = word\n",
    "    word2index['<UNK>'] = index + 1\n",
    "    index2word[index + 1] = '<UNK>'\n",
    "            \n",
    "    return word2index, index2word\n",
    "    \n",
    "# Set the number of most frequent words to create embeddings for. \n",
    "vocabularySize = 50000\n",
    "word2index, index2word = createWordMapper(vocabulary, vocabularySize - 1)\n",
    "\n",
    "print('Size of vocabulary: ' + str(len(word2index)))\n",
    "print('Maximum size of vocabulary: ' + str(len(vocabulary)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec requires the training corpus to be one long document. [data] is one massive array containing the integer representation of all the tweets, stacked after eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "size = len(cleaned_tweets)\n",
    "\n",
    "for tweet in log_progress(cleaned_tweets, size=size):\n",
    "    for word in tweet.split(' '):\n",
    "        #print(word.lower().strip())\n",
    "        word = word.lower().strip()\n",
    "        if word2index.get(word) is not None:\n",
    "            data.append(word2index.get(word))\n",
    "        else:\n",
    "            data.append(word2index.get('<UNK>'))\n",
    "\n",
    "print('The first few words (represented by their integer) of all stacked tweets:')\n",
    "data[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amount of words in total in all tweets')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit tests to the first 10 million words.\n",
    "data = data[:10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at the beginning of the document\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to store the logs for the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/tf/data/twitter/logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate a training batch for the skip-gram model.\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        # context_words is a list of the words around the target word with size 2*skip_window\n",
    "        # [ skip_window target skip_window ]\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        # From our context_words, draw num_skips random words to use as input, that is num_skip of the following\n",
    "        # pairs: [target, context], where context is a random word from context_words\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "    \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# Batch_size is how many to process at a time\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "\n",
    "for i in range(8):\n",
    "    print(batch[i], index2word.get(batch[i]), '->', labels[i, 0],\n",
    "          index2word.get(labels[i, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit\n",
    "# the validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        # Valid dataset is valid_size (number) random integers from the top valid_window (i.e top 100) words.\n",
    "        # That is, those words (integers) is our validation set on which top_k nearest neighbours are found.\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            # Create a random uniformly ditributed [-1.0, 1.0] matrix, with vocabularySize rows and\n",
    "            # embedding_size columns.\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabularySize, embedding_size], -1.0, 1.0))\n",
    "            \n",
    "            # From the embeddings matrix, grab the rows (embeddings) indexed by the train_inputs.\n",
    "            # if train_inputs = [1, 4, 6], then grab row [1, 4, 6] form the embeddings matrix.\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "      # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabularySize, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabularySize]))\n",
    "            \n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabularySize))\n",
    "        \n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    # Grab the embeddings from normalized_embeddings with id/index/row specified in valid_dataset. \n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    # Compute similarity\n",
    "    # [valid_size, embedding_size] * transpose([vocabularySize, embedding_size]) ~\n",
    "    # ~ [valid_size, embedding_size] * [embedding_size, vocabularySize] --> \n",
    "    # --> [valid_size, vocabularySize]. i.e [16, 40'000]\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "num_spest_per_epoch = len(data) // batch_size\n",
    "print(num_spest_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 1500\n",
    "\n",
    "#with tf.Session(graph=graph) as session: #this is required when not using an tf.InteractiveSession()\n",
    "# Open a writer to write summaries.\n",
    "writer = tf.summary.FileWriter(log_dir, session.graph)\n",
    "\n",
    "# We must initialize all variables before we use them.\n",
    "init.run()\n",
    "print('Initialized')\n",
    "\n",
    "average_loss = 0\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                          skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # Define metadata variable.\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    # Also, evaluate the merged op to get all summaries from the returned\n",
    "    # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "    # the graph in TensorBoard.\n",
    "    _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                 feed_dict=feed_dict,\n",
    "                                 run_metadata=run_metadata)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    # Add returned summaries to writer in each step.\n",
    "    writer.add_summary(summary, step)\n",
    "    # Add metadata to visualize the graph for the last run.\n",
    "    if step == (num_steps - 1):\n",
    "        writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "    if step % 10000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 10000\n",
    "        # The average loss is an estimate of the loss over the last 10000\n",
    "        # batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 100000 == 0:\n",
    "        sim = similarity.eval()\n",
    "        for i in range(valid_size):\n",
    "            valid_word = index2word.get(valid_examples[i])\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = index2word.get(nearest[k])\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings and word2index mapper for use in train_eval-w2v.ipynb to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = normalized_embeddings.eval(session=session)\n",
    "\n",
    "pickle.dump(final_embeddings, open(\"w2c-model/final_embeddings.pkl\",\"wb\"))\n",
    "pickle.dump(word2index, open(\"w2c-model/word2index.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(session, os.path.join(log_dir, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
